import os
import time
import re
import json
from openai import OpenAI
from dotenv import load_dotenv

from prompts import PROMPT_BATCH_ANALYSIS, PROMPT_FINAL_MERGE, PROMPT_MIMIC_TRANSLATE

load_dotenv() 

api_key = os.getenv("OPENAI_API_KEY")

if not api_key:
    print("‚ùå –û–®–ò–ë–ö–ê: –ù–µ –Ω–∞–π–¥–µ–Ω API –∫–ª—é—á!")
    print("1. –°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª .env —Ä—è–¥–æ–º —Å main.py")
    print("2. –ù–∞–ø–∏—à–∏—Ç–µ –≤–Ω—É—Ç—Ä–∏: OPENAI_API_KEY=sk-proj-–≤–∞—à_–∫–ª—é—á")
    exit()

client = OpenAI(api_key=api_key)

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
DIRS = {
    "examples": "examples",       
    "style": "style_guide",      
    "input": "to_translate",      
    "output": "output"           
}


ANALYSIS_CHUNK_SIZE = 40000 

TRANSLATION_CHUNK_SIZE = 6000 


def read_txt(path):
    try:
        with open(path, 'r', encoding='utf-8') as f: return f.read()
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {path}: {e}")
        return ""

def save_txt(path, content):
    with open(path, 'w', encoding='utf-8') as f: f.write(content)

def clean_json(text):
    """–ß–∏—Å—Ç–∏—Ç –æ—Ç–≤–µ—Ç –æ—Ç –º–∞—Ä–∫–¥–∞—É–Ω–∞ ```json ... ```"""
    match = re.search(r"```json\s*(.*?)\s*```", text, re.DOTALL)
    if match: return match.group(1)
    return text.replace("```json", "").replace("```", "")

def call_llm(prompt, model="gpt-4o"):
    """
    –£–º–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≤—ã–∑–æ–≤–∞ API.
    –ï—Å–ª–∏ –ª–æ–≤–∏—Ç –æ—à–∏–±–∫—É –ª–∏–º–∏—Ç–æ–≤ (Rate Limit) ‚Äî –∂–¥–µ—Ç –∏ –ø—Ä–æ–±—É–µ—Ç —Å–Ω–æ–≤–∞.
    """
    for attempt in range(3):
        try:
            resp = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.4
            )
            return resp.choices[0].message.content
        except Exception as e:
            err_msg = str(e).lower()
            print(f"‚ö†Ô∏è API Error ({model}, –ø–æ–ø—ã—Ç–∫–∞ {attempt+1}): {e}")
            
            if "rate_limit" in err_msg or "429" in err_msg:
                print("‚è≥ –õ–∏–º–∏—Ç API –ø—Ä–µ–≤—ã—à–µ–Ω. –ñ–¥—É 20 —Å–µ–∫—É–Ω–¥...")
                time.sleep(20)
            else:
                time.sleep(5)
    return ""

def split_text_smart(text, limit):
    """–†–µ–∂–µ—Ç —Ç–µ–∫—Å—Ç –ø–æ –∞–±–∑–∞—Ü–∞–º, –Ω–µ —Ä–∞–∑—Ä—ã–≤–∞—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è."""
    paragraphs = text.split('\n')
    chunks = []
    current_chunk = []
    current_len = 0
    for p in paragraphs:

        if current_len + len(p) > limit and current_chunk:
            chunks.append("\n".join(current_chunk))
            current_chunk = []
            current_len = 0
        current_chunk.append(p)
        current_len += len(p) + 1
    if current_chunk: chunks.append("\n".join(current_chunk))
    return chunks


def perform_deep_scan():
    print("\nüïµÔ∏è –ù–ê–ß–ò–ù–ê–Æ –ì–õ–£–ë–û–ö–û–ï –°–ö–ê–ù–ò–†–û–í–ê–ù–ò–ï –í–°–ï–• –ö–ù–ò–ì...")
    
    files = sorted(os.listdir(DIRS["examples"]))
    sources = [f for f in files if "_source.txt" in f]
    
    all_notes = [] 
    full_reference_text = ""

    if not sources:
        print("‚ùå –ù–µ—Ç —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ examples.")
        return None, None

    total_files = len(sources)
    for idx, src in enumerate(sources):
        tgt = src.replace("_source.txt", "_target.txt")
        if tgt not in files: continue
        
        print(f"   üìñ –ß–∏—Ç–∞—é –∫–Ω–∏–≥—É {idx+1}/{total_files}: {src}...")
        
        s_text = read_txt(os.path.join(DIRS["examples"], src))
        t_text = read_txt(os.path.join(DIRS["examples"], tgt))
        
        if len(full_reference_text) < 5000:
            full_reference_text += t_text[:5000]

        s_chunks = split_text_smart(s_text, ANALYSIS_CHUNK_SIZE)
        t_chunks = split_text_smart(t_text, ANALYSIS_CHUNK_SIZE)
        
        limit = min(len(s_chunks), len(t_chunks))
        
        for i in range(limit):
            print(f"      üî¨ –ê–Ω–∞–ª–∏–∑ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ {i+1}/{limit}...")
            combined_chunk = f"ORIGINAL:\n{s_chunks[i]}\n\nTRANSLATION:\n{t_chunks[i]}"
            
            notes = call_llm(PROMPT_BATCH_ANALYSIS.format(content_chunk=combined_chunk), model="gpt-4o-mini")
            if notes:
                all_notes.append(notes)

    print("\nüß† –û–ë–™–ï–î–ò–ù–Ø–Æ –î–ê–ù–ù–´–ï –í –ï–î–ò–ù–´–ô DNA...")
    
    raw_data = "\n\n=== –ó–ê–ú–ï–¢–ö–ò ===\n".join(all_notes)
    
    try_limits = [160000, 80000, 40000] 
    
    final_json = None
    
    for limit in try_limits:
        print(f"   üîÑ –ü—Ä–æ–±—É—é —Å–æ–±—Ä–∞—Ç—å DNA (–æ–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö: {limit} —Å–∏–º–≤)...")
        safe_data = raw_data[:limit]
        
        final_json_str = call_llm(PROMPT_FINAL_MERGE.format(raw_notes=safe_data), model="gpt-4o-mini")
        cleaned = clean_json(final_json_str)
        
        if cleaned and len(cleaned) > 20 and "Error" not in cleaned:
            final_json = cleaned
            break # –£—Å–ø–µ—Ö!
        else:
            print("   ‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –¥–∞–Ω–Ω—ã—Ö. –£—Ä–µ–∑–∞—é –∏ –ø—Ä–æ–±—É—é —Å–Ω–æ–≤–∞...")

    if not final_json:
        print("‚ùå –û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å DNA. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∞–π–ª—ã –ø—Ä–∏–º–µ—Ä–æ–≤.")
        return None, None

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    dna_path = os.path.join(DIRS["style"], "translator_dna.json")
    save_txt(dna_path, final_json)
    print("‚úÖ MASTER DNA –£–°–ü–ï–®–ù–û –°–û–ó–î–ê–ù.")
    
    return final_json, full_reference_text


def main():

    for d in DIRS.values(): os.makedirs(d, exist_ok=True)
    
    dna_path = os.path.join(DIRS["style"], "translator_dna.json")
    
    if os.path.exists(dna_path):
        print("‚ÑπÔ∏è –ù–∞–π–¥–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π DNA.")
        choice = input("–ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –µ–≥–æ –∑–∞–Ω–æ–≤–æ (—Å–∫–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ –∫–Ω–∏–≥–∏)? (y/n): ")
        if choice.lower() == 'y':
            style_dna, ref_text = perform_deep_scan()
        else:
            style_dna = read_txt(dna_path)
            ref_text = "–¢–µ–∫—Å—Ç –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞ —Å—Ç–∏–ª—è..." 
    else:
        style_dna, ref_text = perform_deep_scan()

    if not style_dna: return

    input_files = [f for f in os.listdir(DIRS["input"]) if f.endswith(".txt")]
    
    if not input_files:
        print(f"üìÇ –ü–∞–ø–∫–∞ {DIRS['input']} –ø—É—Å—Ç–∞. –ü–æ–ª–æ–∂–∏—Ç–µ —Ç—É–¥–∞ .txt —Ñ–∞–π–ª.")
        return

    for filename in input_files:
        print(f"\nüöÄ –ü–ï–†–ï–í–û–î –ö–ù–ò–ì–ò: {filename}")
        full_source = read_txt(os.path.join(DIRS["input"], filename))
        chunks = split_text_smart(full_source, TRANSLATION_CHUNK_SIZE)
        
        full_translation = []
        for i, chunk in enumerate(chunks):
            print(f"   ‚è≥ –ü–µ—Ä–µ–≤–æ–∂—É —á–∞—Å—Ç—å {i+1}/{len(chunks)}...")
            
            prompt = PROMPT_MIMIC_TRANSLATE.format(
                style_json=style_dna,
                reference_sample=ref_text[:2000],
                source_text=chunk
            )
            
            res = call_llm(prompt, model="gpt-4o") 
            res = clean_json(res)
            
            full_translation.append(res)
            
            save_txt(os.path.join(DIRS["output"], f"temp_{filename}"), "\n".join(full_translation))

        final_path = os.path.join(DIRS["output"], filename.replace(".txt", "_RU.txt"))
        save_txt(final_path, "\n".join(full_translation))
        print(f"üèÅ –ì–û–¢–û–í–û! –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {final_path}")

        if os.path.exists(os.path.join(DIRS["output"], f"temp_{filename}")):
            os.remove(os.path.join(DIRS["output"], f"temp_{filename}"))

if __name__ == "__main__":
    main()
